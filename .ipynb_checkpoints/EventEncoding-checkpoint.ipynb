{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-17T21:02:52.725537Z",
     "start_time": "2018-05-17T21:02:52.714730Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/media/gu38/VariousDB/DARPA/CombinedTask/RawFiles/\")\n",
    "len(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-17T21:02:59.795401Z",
     "start_time": "2018-05-17T21:02:58.583335Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1526590978.657394\n",
      "1420088445\n",
      "Time Elapsed: 1.136509656906128\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import glob\n",
    "import json\n",
    "import nltk\n",
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "timenow0 = time.time()\n",
    "print(timenow0)\n",
    "def process_timestamp(timeString, granul):\n",
    "    timestamp = int(time.mktime(datetime.datetime.strptime(\n",
    "        timeString, \"%Y-%m-%d\" + \"T\" + \"%H:%M:%S\" + \"Z\").timetuple()))\n",
    "    print(timestamp)\n",
    "    \n",
    "def generate_time_vectors(fileName, masterDict, prevTstamps, gran):\n",
    "#     start_time = time.time()\n",
    "    fileNumx = str(fileName)[-11:]\n",
    "    eventType = str(fileName)[:-11]\n",
    "    temp_mod_event = {}\n",
    "    \n",
    "    with open(fileName,'r') as events:\n",
    "        temp_mod_event = json.load(events)\n",
    "        \n",
    "        eventList = []\n",
    "        for ID, event in temp_mod_event.items():\n",
    "            user = event['actor_id_h']\n",
    "            if user in masterDict:\n",
    "                time_ms = int(time.mktime(datetime.datetime.strptime(event['created_at'],\n",
    "                \"%Y-%m-%d\" + \"T\" + \"%H:%M:%S\" + \"Z\").timetuple()))\n",
    "                prevTstamps[user][eventType] = process_timestamp()\n",
    "                \n",
    "    \n",
    "\n",
    "\n",
    "granularity = 'hourly' #could take hourly/minute/second or day/month/year as values)\n",
    "masterDictionary = {} # To store the final time vectors\n",
    "prevTimeStamps = {} # To store the timestamps of previous event of certain type\n",
    "\n",
    "for file in os.listdir(os.getcwd()):\n",
    "    if '00000' in file:\n",
    "        generate_time_vectors(file, masterDictionary, prevTimeStamps, granularity)\n",
    "process_timestamp(\"2015-01-01T00:00:45Z\", \"yeah\")\n",
    "        \n",
    "print(\"Time Elapsed:\", str(time.time() - timenow0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-17T21:03:06.202498Z",
     "start_time": "2018-05-17T21:03:01.400405Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2196017 300\n",
      ", -0.082752 0.67204 -0.14987 -0.064983 0.056491 0.40228 0.0027747 -0.3311 -0.306\n",
      "91 2.0817 0.031819 0.013643 0.30265 0.0071297 -0.5819 -0.2774 -0.062254 1.1451 -\n",
      "0.24232 0.1235 -0.12243 0.33152 -0.006162 -0.30541 -0.13057 -0.054601 0.037083 -\n",
      "0.070552 0.5893 -0.30385 0.2898 -0.14653 -0.27052 0.37161 0.32031 -0.29125 0.005\n",
      "2483 -0.13212 -0.052736 0.087349 -0.26668 -0.16897 0.015162 -0.0083746 -0.14871 \n",
      "0.23413 -0.20719 -0.091386 0.40075 -0.17223 0.18145 0.37586 -0.28682 0.37289 -0.\n",
      "16185 0.18008 0.3032 -0.13216 0.18352 0.095759 0.094916 0.008289 0.11761 0.34046\n",
      " 0.03677 -0.29077 0.058303 -0.027814 0.082941 0.1862 -0.031494 0.27985 -0.074412\n",
      " -0.13762 -0.21866 0.18138 0.040855 -0.113 0.24107 0.3657 -0.27525 -0.05684 0.34\n",
      "872 0.011884 0.14517 -0.71395 0.48497 0.14807 0.62287 0.20599 0.58379 -0.13438 0\n",
      ".40207 0.18311 0.28021 -0.42349 -0.25626 0.17715 -0.54095 0.16596 -0.036058 0.08\n",
      "499 -0.64989 0.075549 -0.28831 0.40626 -0.2802 0.094062 0.32406 0.28437 -0.26341\n",
      " 0.11553 0.071918 -0.47215 -0.18366 -0.34709 0.29964 -0.66514 0.002516 -0.42333 \n",
      "0.27512 0.36012 0.16311 0.23964 -0.05923 0.3261 0.20559 0.038677 -0.045816 0.089\n",
      "764 0.43151 -0.15954 0.08532 -0.26572 -0.15001 0.084286 -0.16714 -0.43004 0.0608\n",
      "07 0.13121 -0.24112 0.66554 0.4453 -0.18019 -0.13919 0.56252 0.21457 -0.46443 -0\n",
      ".012211 0.029988 -0.051094 -0.20135 0.80788 0.47377 -0.057647 0.46216 0.16084 -0\n",
      ".20954 -0.05452 0.15572 -0.13712 0.12972 -0.011936 -0.003378 -0.13595 -0.080711 \n",
      "0.20065 0.054056 0.046816 0.059539 0.046265 0.17754 -0.31094 0.28119 -0.24355 0.\n",
      "085252 -0.21011 -0.19472 0.0027297 -0.46341 0.14789 -0.31517 -0.065939 0.036106 \n",
      "0.42903 -0.33759 0.16432 0.32568 -0.050392 -0.054297 0.24074 0.41923 0.13012 -0.\n",
      "17167 -0.37808 -0.23089 -0.019477 -0.29291 -0.30824 0.30297 -0.22659 0.081574 -0\n",
      "\u001b[Km--More--(0%)\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!more  ../glove.840B.300d.w2v.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-05-17T21:20:17.138Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import glob\n",
    "import json\n",
    "import nltk\n",
    "import gensim\n",
    "import numpy as np\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "timenow = time.time()\n",
    "\n",
    "# The current directory needs to be the one where all the original JSON files are located\n",
    "# wordvec representation of NLP vector attributes, adjust the directory accordingly\n",
    "word_vectors = KeyedVectors.load_word2vec_format('../glove.840B.300d.w2v.txt', binary=False)\n",
    "\n",
    "# Now, let's create an additional directory parallel to the original directory\n",
    "# to hold their binary+NLP encoded version:\n",
    "os.makedirs(os.getcwd() + os.sep + os.pardir + \"/Encoded/\", exist_ok=True)\n",
    "\n",
    "# Bit-Encoding for binary representable attributes\n",
    "def encoder(filename):\n",
    "#     start_time = time.time()\n",
    "    fileNumx = str(filename)[-11:]\n",
    "    eventType = str(filename)[:-11]\n",
    "    temp_mod_event = {}\n",
    "    \n",
    "    with open(filename,'r') as events:\n",
    "        temp_mod_event = json.load(events)\n",
    "        \n",
    "        eventList = []\n",
    "        for ID, event in temp_mod_event.items():\n",
    "            eventList.append(event)\n",
    "            \n",
    "        if len(temp_mod_event) == 0: #For Emplty files\n",
    "            with open(os.getcwd() + os.sep + os.pardir + '/Encoded/' + eventType + 'mod' + fileNum, 'w') as outfile:\n",
    "                json.dump(temp_mod_event, outfile)\n",
    "            return\n",
    "            \n",
    "        if eventType == \"CreateEvent\":\n",
    "            nlp_attributes = [\"payload_description_m\"]\n",
    "            bit_attributes = {'payload_ref_type': ['branch', 'repository', 'tag'], \n",
    "                                 'payload_pusher_type': ['deploy_key', 'user'],\n",
    "                                 'type': ['CreateEvent', 'DeleteEvent', 'ForkEvent', 'IssuesEvent', \n",
    "                                          'PullRequestEvent', 'PushEvent', 'WatchEvent']}\n",
    "        if eventType == \"DeleteEvent\":\n",
    "            nlp_attributes = []\n",
    "            bit_attributes = {'payload_ref_type': ['branch', 'tag'],\n",
    "                                 'payload_pusher_type': ['user'],\n",
    "                                 'type': ['CreateEvent', 'DeleteEvent', 'ForkEvent', 'IssuesEvent', \n",
    "                                          'PullRequestEvent', 'PushEvent', 'WatchEvent']}\n",
    "        if eventType == \"ForkEvent\":\n",
    "            nlp_attributes = [\"payload_forkee_description_m\"]\n",
    "            bit_attributes = {\n",
    "                                 'payload_forkee_has_pages': ['True', 'False'],\n",
    "                                 'payload_forkee_has_issues': ['True', 'False'],\n",
    "                                 'payload_forkee_public': ['True', 'False'],\n",
    "                                 'payload_forkee_has_downloads': ['True', 'False'],\n",
    "                                 'payload_forkee_owner_type': ['Organization', 'User'],\n",
    "                                 'payload_forkee_owner_site_admin': ['True', 'False'],\n",
    "                                 'payload_forkee_has_wiki': ['True', 'False'],\n",
    "                                 'type': ['CreateEvent', 'DeleteEvent', 'ForkEvent', 'IssuesEvent', \n",
    "                                          'PullRequestEvent', 'PushEvent', 'WatchEvent']}\n",
    "        if eventType == \"IssuesEvent\":\n",
    "            nlp_attributes = [\"payload_issue_title_m\",\"payload_issue_body_m\",\"payload_issue_milestone_title_m\"]\n",
    "            bit_attributes = {'payload_issue_user_type': ['Organization', 'User'],\n",
    "                                  'payload_issue_locked': ['True', 'False'],\n",
    "                                  'payload_issue_state': ['closed', 'open'],\n",
    "                                  'payload_action': [\"assigned\", \"closed\", \"demilestoned\", \"edited\", \"labeled\", \n",
    "                                                     \"milestoned\", \"opened\", \"reopened\", \"unassigned\", \"unlabeled\"],\n",
    "                                  'payload_issue_user_site_admin': ['True', 'False'],\n",
    "                                 'type': ['CreateEvent', 'DeleteEvent', 'ForkEvent', 'IssuesEvent', \n",
    "                                          'PullRequestEvent', 'PushEvent', 'WatchEvent']}\n",
    "        if eventType == \"PullRequestEvent\":\n",
    "            nlp_attributes = [\"payload_pull_request_body_m\",\"payload_pull_request_title_m\",\"payload_pull_request_milestone_title_m\",\n",
    "                         \"payload_pull_request_milestone_description_m\"]\n",
    "            bit_attributes = {'payload_action': [\"assigned\", \"unassigned\", \"review_requested\", \"review_request_removed\", \n",
    "                                                    \"labeled\", \"unlabeled\", \"opened\", \"edited\", \"closed\", \"reopened\"],\n",
    "                                 'payload_pull_request_head_repo_owner_type': ['Organization', 'User'],\n",
    "                                 'payload_pull_request_base_repo_owner_site_admin': ['True', 'False'],\n",
    "                                 'payload_pull_request_base_repo_has_wiki': ['True', 'False'],\n",
    "                                 'payload_pull_request_locked': ['True', 'False'],\n",
    "                                 'payload_pull_request_user_type': ['Organization', 'User'],\n",
    "                                 'payload_pull_request_head_repo_has_pages': ['True', 'False'],\n",
    "                                 'payload_pull_request_milestone_creator_site_admin': ['True', 'False'],\n",
    "                                 'payload_pull_request_milestone_state': ['closed', 'open'],\n",
    "                                 'payload_pull_request_base_repo_private': ['True', 'False'],\n",
    "                                 'payload_pull_request_mergeable': ['True', 'False'],\n",
    "                                 'payload_pull_request_base_user_type': ['Organization', 'User'],\n",
    "                                 'payload_pull_request_head_repo_private': ['True', 'False'],\n",
    "                                 'payload_pull_request_base_repo_owner_type': ['Organization', 'User'],\n",
    "                                 'payload_pull_request_mergeable_state': ['clean', 'dirty', 'unknown', 'unstable'],\n",
    "                                 'payload_pull_request_base_repo_has_pages': ['True', 'False'],\n",
    "                                 'payload_pull_request_milestone_creator_type': ['Organization', 'User'],\n",
    "                                 'payload_pull_request_head_repo_owner_site_admin': ['True', 'False'],\n",
    "                                 'payload_pull_request_base_repo_has_issues': ['True', 'False'],\n",
    "                                 'payload_pull_request_head_repo_has_downloads': ['True', 'False'],\n",
    "                                 'payload_pull_request_head_repo_has_issues': ['True', 'False'],\n",
    "                                 'payload_pull_request_state': ['closed', 'open'],\n",
    "                                 'payload_pull_request_base_user_site_admin': ['True', 'False'],\n",
    "                                 'payload_pull_request_user_site_admin': ['True', 'False'],\n",
    "                                 'payload_pull_request_merged': ['True', 'False'],\n",
    "                                 'payload_pull_request_head_user_type': ['Organization', 'User'],\n",
    "                                 'payload_pull_request_head_repo_has_wiki': ['True', 'False'],\n",
    "                                 'payload_pull_request_head_user_site_admin': ['True', 'False'],\n",
    "                                 'payload_pull_request_base_repo_has_downloads': ['True', 'False'],\n",
    "                                 'type': ['CreateEvent', 'DeleteEvent', 'ForkEvent', 'IssuesEvent', \n",
    "                                          'PullRequestEvent', 'PushEvent', 'WatchEvent']}\n",
    "        if eventType == \"PushEvent\":\n",
    "            nlp_attributes = [[\"payload_commits\",\"message_m\"]]    \n",
    "            bit_attributes = {'payload_commits': [],\n",
    "                                  'type': ['CreateEvent', 'DeleteEvent', 'ForkEvent', 'IssuesEvent', \n",
    "                                      'PullRequestEvent', 'PushEvent', 'WatchEvent']}\n",
    "        if eventType == \"WatchEvent\":\n",
    "            nlp_attributes = []\n",
    "            bit_attributes = {'payload_action': ['started'],\n",
    "                                 'type': ['CreateEvent', 'DeleteEvent', 'ForkEvent', 'IssuesEvent', \n",
    "                                          'PullRequestEvent', 'PushEvent', 'WatchEvent']}\n",
    "        \n",
    "        #generic interesting attributes that we do not encode\n",
    "        num_attributes = [\"actor_login_h\",\"repo_name_h\",\"created_at_total_seconds\",\"created_at\"]\n",
    "\n",
    "        temp_mod_event = {} #Dictionary to hold all the encoded and interesting attributes\n",
    "        for event in eventList:\n",
    "            ID = event['id_h']\n",
    "            temp_mod_event[ID] = {}\n",
    "            \n",
    "            for param in bit_attributes:\n",
    "                if param in event:\n",
    "                    entry = event[param]\n",
    "                    \n",
    "                    #Actual binary encoding implementation\n",
    "                    encodedEntry = []\n",
    "                    for value in bit_attributes[param]:\n",
    "                        if (str(entry).lower() == value.lower()):\n",
    "                            encodedEntry.append(1)\n",
    "                        else:\n",
    "                            encodedEntry.append(0)\n",
    "                                \n",
    "                else:\n",
    "                    encodedEntry = [0] * len(bit_attributes[param])\n",
    "                    \n",
    "                temp_mod_event[ID][param] = encodedEntry\n",
    "                    \n",
    "                if eventType == \"PushEvent\":\n",
    "                    if param == \"payload_commits\":\n",
    "                        payload_commits = event[param]\n",
    "                        pc = []\n",
    "\n",
    "                        for commits in payload_commits:\n",
    "                            pcs = {}\n",
    "                            distinct = [1, 0] if commits['distinct'] else [0, 1]\n",
    "\n",
    "                            pcs['distinct'] = distinct\n",
    "                            pcs['message_m'] = commits['message_m']\n",
    "\n",
    "                            pc.append(pcs)\n",
    "\n",
    "                        temp_mod_event[ID]['payload_commits'] = pc\n",
    "                        \n",
    "                        \n",
    "            for attr in nlp_attributes:      #For each NLP attribute present in the event type\n",
    "                if eventType != \"PushEvent\":\n",
    "                    try:\n",
    "                        #tokenize the text present\n",
    "                        temp_text = [k.lower() for k in list(nltk.word_tokenize(event[attr]))] \n",
    "                    except KeyError:\n",
    "                        temp_mod_event[attr] = np.zeros((300,)).tolist()\n",
    "                        continue\n",
    "                    if len(temp_text)<1:\n",
    "                        wv = np.zeros((300,))\n",
    "                    else:\n",
    "                        wv = np.zeros((300,))\n",
    "                        remcount = 0\n",
    "                        for text in temp_text:          #Get word vectors using loaded wordvec model                        \n",
    "                            try:\n",
    "                                wv = wv + word_vectors[text]    \n",
    "                            except KeyError:\n",
    "                                remcount += 1\n",
    "                                pass\n",
    "\n",
    "                        wv = wv/(len(temp_text)-remcount)      #Get average of word vectors                    \n",
    "                    temp_mod_event[ID][attr] = wv.tolist()\n",
    "                else:\n",
    "                    commits = temp_mod_event[ID][attr[0]]\n",
    "                    rem_commits_count = 0\n",
    "                    for x in range(0,len(commits)):\n",
    "                        try:\n",
    "                            temp_text = [k.lower() for k in list(nltk.word_tokenize(commits[x][attr[1]]))] #tokenize \n",
    "                        except KeyError:\n",
    "                            commits[x][attr[1]] = np.zeros((300,)).tolist() #Put zeros if commits do not have messages\n",
    "                            rem_commits_count += 1 #Count of commmits without messages\n",
    "                            continue\n",
    "                        if len(temp_text)<1:\n",
    "                            wv = np.zeros((300,)) #Put zeros if attirbute 'message' is present but without words\n",
    "                        else:\n",
    "                            wv = np.zeros((300,))\n",
    "                            remcount = 0\n",
    "                            for text in temp_text:          #Get word vectors using loaded wordvec model                        \n",
    "                                try:\n",
    "                                    wv = wv + word_vectors[text]    \n",
    "                                except KeyError:\n",
    "                                    remcount += 1 #Words that do not have wordvec in the model\n",
    "                                pass\n",
    "\n",
    "                            wv = wv/(len(temp_text)-remcount)\n",
    "                        commits[x][attr[1]] = wv.tolist()  \n",
    "\n",
    "                    temp_mod_event[ID][attr[0]] = commits    \n",
    "            for param in num_attributes:\n",
    "                temp_mod_event[ID][param] = event[param]\n",
    "                                    \n",
    "        with open(os.getcwd() + os.sep + os.pardir + \"/Encoded/\" + eventType + 'mod' + fileNumx, 'w') as outfile:\n",
    "            json.dump(temp_mod_event, outfile)\n",
    "\n",
    "for file in os.listdir(os.getcwd()):\n",
    "    if 'json' in file:\n",
    "        encoder(file)\n",
    "        \n",
    "print(\"Time Elapsed:\", str(time.time() - timenow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T20:34:38.334513Z",
     "start_time": "2018-05-02T20:34:38.307412Z"
    }
   },
   "outputs": [],
   "source": [
    "arr1 = [1, 2, 3]\n",
    "arr2 = [2, 3, 1]\n",
    "\n",
    "count = 0\n",
    "for i in range(len(arr1)):\n",
    "    if(arr1[i] < arr2[i]):\n",
    "        count += 1\n",
    "\n",
    "if(count == len(arr1)):\n",
    "    print(\"Greater for sure\")\n",
    "else:\n",
    "    print(\"unsure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-03T00:20:41.926149Z",
     "start_time": "2018-05-03T00:20:41.916654Z"
    }
   },
   "outputs": [],
   "source": [
    "dict = {1,2,3}\n",
    "for item in dict:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
